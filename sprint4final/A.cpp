//https://contest.yandex.ru/contest/24414/run-report/106637086/

/*
-- ПРИНЦИП РАБОТЫ --

для быстрого поиска по сохранённым документам используется std::unordered_map хеш-таблица из стандартной библиоитеки, 
в которой ключ - это слово, а значение это вложенная хеш-таблица, в которой хранятся информация о документах, 
где и как часто это слово встречается, т.е. ключ - это номер документа, значение - частота слова в документе

при обработке запросов создаётся хеш-таблица, 
которая по номеру документа хранит релевантность запроса - то как часто слова из запроса встречаются в документе

перебираются все уникальные слова из запроса, 
и для каждого уникального слова из запроса происходит выбор значения из хеш-таблицы с документами (по слову)
и высчитывается релевантность каждого слова - частота этого слова в каждом документе

-- ДОКАЗАТЕЛЬСТВО КОРРЕКТНОСТИ --

в хеш-таблице, которая по номеру документа хранит релевантность запроса, уже нужные данные, 
но по заданию их нужно отсортировать по убыванию релевантности 
и вывести не более 5 номеров документов, данные в хеш-таблице не упорядочены, 
поэтому данные из хеш-таблицы перекладываются в вектор и сортируются по релевантности и затем по номеру документа

-- ВРЕМЕННАЯ СЛОЖНОСТЬ --

unordered_map - если хеш-функция достаточно равномерна и корзин достаточно много, 
то в среднем время поиска, добавления и удаления элементов для unordered_map будет константным  O(1)
https://education.yandex.ru/handbook/cpp/article/associative-containers
перебор всех слов в документах это линейная сложность O(n1*w1), зависит от количества документов n1 и слов в документах w1,
перебор всех слов из запроса тоже линейная сложность O(n2*w2), зависит от количества запросов n2 и уникальных слов в запросе w2,
здесь n*w - это постоянный коэффицциент, который зависит от числа документов (строк) и числа слов в документе (строке),
т.е O(n*w) это линейная зависимость, т.е. сложность прямопропорционально растёт в зависимости от количества документов и слов в них,
перебор всех полученных релевантных значений из хеш таблицы и копирование слов в вектор O(n3),
сортировка этого ветктора за O(n3*log(n3)), где n3 это размер вектора
https://en.cppreference.com/w/cpp/algorithm/sort
O(n3*log(n3)) больше O(n3), поэтому O(n3) можно не учитвать,
также вектор будет создаваться и сортироваться в цикле по числу запросов n2, т.е. O(n2*n3*log(n3)),
размер ветора, всегда будет меньше или равен числу документов n3 <= n1, т.е. в худшем случае O(n2*n1*log(n1)),
т.е. в худшем случае, если много документов, и приходит много запросов, для которых находятся слова в каждом документе, то сложность будет O(n*log(n))
поэтому итоговая временная сложность будет O(n*log(n))

-- ПРОСТРАНСТВЕННАЯ СЛОЖНОСТЬ --

векторы, передаётся по ссылке, они не копируется, поэтому их размер не учитывается,
хеш-таблица для хранения слов из документов зависит от количества документов n1 и числа уникальных слов в документах w1, поэтому O(n1*w1),
на каждый запрос n2 в цикле на стеке выделяется память:
- сет для хранения уникальных слов w2 из запроса, поэтому O(w2),
- хеш-таблица для хранения релевантных значений на запрос, в худшем случае размер равен числу документов n1, поэтому O(n1)
- вектор для сортировки значений, в худшем случае размер равен числу документов n1, поэтому O(n1)
после цикла память выделенная в цикле на стеке освобождается,
n*w - это постоянный коэффицциент, который зависит от числа документов (строк) и числа слов в документе (строке),
т.е O(n*w) это линейная зависимость, т.е. сложность прямопропорционально растёт в зависимости от количества документов и слов в них,
т.е. худший случай когда много документов, в них много слов и в запросе много слов, то это O(n1*w1 + w2 + n1 + n1)=O(n1*w1 + w2), 
т.е. будет O(число слов в документах + число слов в запросе), а это линейная сложность,
поэтму итоговая простравнственная сложность линейная O(n)
*/

#include <algorithm>
#include <iostream>
#include <iterator>
#include <unordered_map>
#include <set>
#include <sstream>
#include <string>
#include <vector>

using namespace std;

// надо вывести 5 самых релевантных документов
const int MAX_QUERY = 5;

// возвращает вектор по числу запросов, который содержит вектор, 
// который содержит пару <релевантность запроса (количество совпадений слов из запроса со словами в документе), номер документа>
vector<vector<pair<int, int>>> getTopIdDocuments(vector<string>& documents, vector<string>& queries) {
	// хеш таблица с ключом по слову, значение - хеш-таблица с ключом номеру документа, он же id, значение - счётчик слов в этом документе, он же релелевантность слова
	// т.е. для каждого уникального слова образуется набор уникальных пар по номеру документа:
	// <слово, { <номер документа, релевантность (частота) слова в документе> } >
	unordered_map<string, unordered_map<int, int>> documentsHashTable;
	for (int i = 0; i < documents.size(); ++i) {
		// имеется n документов, каждый из которых представляет собой текст из слов
		stringstream ss(documents[i]);
		string word;
		while (ss >> word) {
			// если одно слово много раз встречается в одном документе, то его релевантность выше
			// для простоты дальнейшей сортировки по убыванию релевантности, счётчик слов (релевантность) будет со знаком минус (-1,-2,-3..)
			--documentsHashTable[word][i + 1];
		}
	}
	vector<vector<pair<int, int>>> result;
	for (const auto& query : queries) {
		// запрос —– некоторый набор слов
		istringstream ss(query);
		istream_iterator iter = istream_iterator<string>(ss);
		// нужны уникальные слова из запроса, поэтому используется set
		set<string> querySet(iter, istream_iterator<string>());
		// хеш-таблица для запроса <ключ: id документа, значение: релевантность запроса - как часто слова из запроса встречаются в документе с ключом id>
		unordered_map<int, int> ids;
		// перебор уникальных слов из запроса
		for (const auto& word : querySet) {
			if (documentsHashTable.find(word) != documentsHashTable.end()) {
				for (const auto& [id, relevance] : documentsHashTable[word]) {
					// релевантность документа оценивается следующим образом: 
					// для каждого уникального слова из запроса берётся число его вхождений в документ, полученные числа для всех слов из запроса суммируются
					ids[id] += relevance;
				}
			}
		}
		if (ids.size()) {
			vector<pair<int, int>> temp;
			for (const auto& [id, relevance] : ids)
				temp.push_back({ relevance, id });
			// сортировка документов на выдаче производится по убыванию релевантности
			// если релевантности документов совпадают —– то по возрастанию их порядковых номеров в базе
			sort(temp.begin(), temp.end());
			// надо вывести 5 самых релевантных документов
			if (temp.size() > MAX_QUERY)
				temp.resize(MAX_QUERY);
			result.push_back(temp);
		}
	}
	return result;
}

int main() {
	int n;
	cin >> n;
	cin.ignore();
	vector<string> documents(n);
	for (int i = 0; i < n; ++i) {
		getline(cin, documents[i]);
	}
	int m;
	cin >> m;
	cin.ignore();
	vector<string> queries(m);
	for (int i = 0; i < m; ++i) {
		getline(cin, queries[i]);
	}
	for (const auto& idDocuments : getTopIdDocuments(documents, queries)) {
		for (const auto& [relevance, id] : idDocuments)
			cout << id << " ";
		cout << endl;
	}
}
